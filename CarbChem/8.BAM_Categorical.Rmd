---
title: "BAM_Categorical"
author: "Ana Palacio-Castro"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    fig_height: 8
    fig_width: 8
    df_print: paged
    toc: yes
editor_options:
  chunk_output_type: console
---


```{r setup, include = FALSE}
  knitr::opts_chunk$set(warning = FALSE, 
                        message = FALSE,
                        #options(knitr.kable.NA = ''),
                        fig.align = "center")
```


```{r ibraries, include=FALSE}
library('mgcv')
library('ggplot2')
library('viridis')
theme_set(theme_bw())
library('gganimate')
library(plyr)
library(dplyr)
```

# Data 

```{r, include=FALSE}
# 1. Get data
WS.data<-read.csv("WS.data.csv", header = T)

# Separate CalVal and Walton Smith data
summary(WS.data$Mission)

```

## Create time variables for the GAM

```{r}
WS.data <- transform(WS.data,
                       datetime = as.POSIXct(paste(ESTDate, EST.Time),
                                             format = '%m/%d/%Y %I:%M:%S %p', tz = "EST"))
summary(WS.data$datetime)
WS.data <- transform(WS.data,
                       SiteID = factor(SiteID),
                       MoY = as.numeric(format(datetime, format = '%m')),
                       #DoY = as.numeric(format(datetime, format = '%j')),
                       ToD = as.numeric(format(datetime, format = '%H')) +
                         (as.numeric(format(datetime, format = '%M')) / 60))
```


## Subset FLK data by zone to use them as endpoints

```{r}
  WS.open<-subset(WS.data, WS.data$Zone=="Offshore"|WS.data$Zone=="Oceanic")
  WS.open$Date<-as.Date(WS.open$Date, "%Y-%m-%d")
 
  WS.reef<-subset(WS.data, WS.data$Zone=="Inshore"|WS.data$Zone=="Mid channel")
  WS.reef$Date<-as.Date(WS.reef$Date, "%Y-%m-%d")
```

* Get the oceanic / offshore mean values per monty-year and substract them from the reef data

```{r}

# Reef data
nTA.MY.open <- ddply (WS.open, .(Sub_region, MY),summarise,
                OnTA_my = mean (nTA, na.rm = T),
                #sd = sd (nTA, na.rm = T),
                number = n())

nDIC.MY.open <- ddply (WS.open, .(Sub_region, MY),summarise,
                OnDIC_my = mean (nDIC, na.rm = T),
                #sd = sd (nDIC, na.rm = T),
                number = n())

# ONly reef
  WS.reef<-join(WS.reef,nTA.MY.open[,-4], 
                   type = "left", by=c("MY", "Sub_region"))
  
  WS.reef<-join(WS.reef, nDIC.MY.open, 
                   type = "left", by=c("MY", "Sub_region"))
  
  # Delta nTA
  WS.reef$dnTA_MY<-WS.reef$nTA-WS.reef$OnTA_my
  
  # Delta nDIC
  WS.reef$dnDIC_MY<-WS.reef$nDIC-WS.reef$OnDIC_my


# all WS data
  WS.data<-join(WS.data,nTA.MY.open[,-4], 
                   type = "left", by=c("MY", "Sub_region"))
  
  WS.data<-join(WS.data, nDIC.MY.open, 
                   type = "left", by=c("MY", "Sub_region"))
  
  # Delta nTA
  WS.data$dnTA_MY<-WS.data$nTA-WS.data$OnTA_my
  
  # Delta nDIC
  WS.data$dnDIC_MY<-WS.data$nDIC-WS.data$OnDIC_my

```

**Quick plote / note about Time of the day:** Either hour of the day does not matter for temperature (lots of circulation?) or times are not very accurate (Waton Smith UTC reporting not always followed?) 

```{r}
ggplot(WS.data, aes(ToD, Temperature_C)) +
  geom_smooth() +
  #geom_line() +
  geom_point(aes(colour=Zone)) +
  theme(panel.border = element_blank(),
        panel.background = element_blank(),
        panel.grid.minor = element_line(colour = "grey90"),
        panel.grid.major = element_line(colour = "grey90"),
        panel.grid.major.x = element_line(colour = "grey90"),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12, face = "bold")) + facet_wrap(~MoY)
```

# Generalized additive models (GAMs) for fitting irregular time series

## Notes

The principle behind GAMs is similar to that of regression, except that instead of summing effects of individual predictors, GAMs are a sum of smooth functions. Functions allow us to model more complex patterns, and they can be averaged to obtain smoothed curves that are more generalizable. Because GAMs are based on functions rather than variables, they are not restricted by the linearity assumption in regression that requires predictor and outcome variables to move in a straight line. Furthermore, unlike in neural networks, we can isolate and study effects of individual functions in a GAM on resulting predictions.

* model the response variable by independent variables, which are in the form of some smooth functions
* Smooth functions are also called splines. Smoothing splines are real functions that are piecewise-defined by polynomial functions (basis functions). The places, where the polynomial pieces connect are called knots. In GAMs, penalized regression splines are used in order to regularize the smoothness of a spline.


There are several smoothing bases b (splines) which are suitable for regression:
* thin plate regression
* cubic regression
* cyclic cubic regression 
* P-splines

bs="tp": Low rank isotropic smoothers of any number of covariates. 

* Isotropic = rotation of the covariate co-ordinate system will not change the result of smoothing
* Low rank = they have far fewer coefficients than there are data to smooth. They are reduced rank versions of the thin plate splines and use the thin plate spline penalty. 
* They are the default smooth for s terms because there is a defined sense in which they are the optimal smoother of any given basis dimension/rank (Wood, 2003).
* Thin plate regression splines do not have ‘knots’ (at least not in any conventional sense): a truncated eigen-decomposition is used to achieve the rank reduction. See tprs for further details.

bs="ts" is as "tp" but with a modification to the smoothing penalty, so that the null space is also penalized slightly and the whole term can therefore be shrunk to zero.

Duchon splines: bs="ds". LONGITUD AND LATITUD
* Thin plate splines. For any given number of covariates they allow lower orders of derivative in the penalty than thin plate splines (and hence a smaller null space). See Duchon.spline for further details.


Smoothing parameter (λ) and the number of basis dimensions (i.e. degrees of freedom):
* GAM uses Generalized Cross Validation score (GCV) to minimize vg
* when lambda is near 1 then spline will be over-smoothed
* when lambda is near zero than spline isn’t penalized, so the method behaves like a classical OLS.
* With a number of basis dimensions (estimated degrees of freedom), it is opposite. Higher dimension implies that fit will be less smoothed (overfit), on the other side lower dimensions implies more smoothed behavior of fitted values.

Interactions:
* Multiplication of two independent variables
* smoothed function to one variable
* same smoothed function for both variables: 
* tensor product interactions: different smoothing bases for variables and penalize it in two (when we do interactions of two independent variables) different ways: 



# GAM models with bam

All these models below include location as coordinates and not as categories in Zone (In- off-shore. etc), UK, MK and LK. That might be better or not?

Number of years, monts (of the year) and time of the day (useless) in the data

```{r model parameters}

n_Year <- unique(WS.data[, "Year"])
length(n_Year) 

n_MoY <- unique(WS.data[, "MoY"])
length(n_MoY)

n_ToD <- unique(WS.data [, "ToD"])
length(n_ToD)

N <- nrow(WS.data) # number of observations in the train set
period<-N/(length(n_Year))/12
period<-round(period)
window <- N / period # number of days in the train set

knots <- list(DoY = c(0.5, 366.5))
M <- list(c(1, 0.5), NA)

```

# All data - BAM: Generalized additive models for very large datasets 

### Temperature

```{r}
# 1. Model

m_0 <- bam(Temperature_C ~ Sub_region +
           s(ToD, k = 12, bs = 'cc') + # Time of the day//added bs='cr'
           s(MoY, k = 6, bs = 'cc', by=Zone) +  # Month of the year?
           s(Year, by=Zone) +
           #s(Year, SiteID, bs = "fs", m = 1, k = 5)+
           #s(Zone, bs = 'fs', k=4)
           ti(MoY, Year, bs = c('cc', 'tp'),k = c(6, 5)), # +
           #ti(Zone, ToD, bs = c('fs','tp'), k = c(4, 12)) +
           #ti(Zone, MoY, bs = c('fs','cc'), k = c(4, 6)) +
           #ti(Zone, Year, bs = c('fs','tp'), k = c(4, 5)),
           data = WS.data, method = 'fREML', knots = knots, nthreads = 4, discrete = TRUE)
summary(m_0)

plot(m_0, pages = 1, scheme = 2, shade = TRUE)


m_1 <- bam(Temperature_C ~ Zone +
           s(ToD, k = 12, bs = 'cc') + # Time of the day//added bs='cr'
           s(MoY, k = 6, bs = 'cc') +  # Month of the year?
           s(Year, by=Sub_region) +
           #s(Year, SiteID, bs = "fs", m = 1, k = 5)+
           #s(Zone, bs = 'fs', k=4)
           ti(MoY, Year, bs = c('cc', 'tp'),k = c(6, 5)), # +
           #ti(Zone, ToD, bs = c('fs','tp'), k = c(4, 12)) +
           #ti(Zone, MoY, bs = c('fs','cc'), k = c(4, 6)) +
           #ti(Zone, Year, bs = c('fs','tp'), k = c(4, 5)),
           data = WS.data, method = 'fREML', knots = knots, nthreads = 4, discrete = TRUE)
summary(m_1)
plot(m_1, pages = 1, scheme = 2, shade = TRUE)


m_2 <- bam(Temperature_C ~ 
           #s(ToD, k = 12, bs = 'cc') + # Time of the day//added bs='cr'
           s(MoY, k = 6, bs = 'cc') +  # Month of the year?
           s(Year, by=Zone) +
           #s(Year, SiteID, bs = "fs", m = 1, k = 5)+
           #s(Zone, bs = 'fs', k=4)
           ti(MoY, Year, bs = c('cc', 'tp'),k = c(6, 5)), # +
           #ti(Zone, ToD, bs = c('fs','tp'), k = c(4, 12)) +
           #ti(Zone, MoY, bs = c('fs','cc'), k = c(4, 6)) +
           #ti(Zone, Year, bs = c('fs','tp'), k = c(4, 5)),
           data = WS.data, method = 'fREML', knots = knots, nthreads = 4, discrete = TRUE)
summary(m_2)
plot(m_2, pages = 1, scheme = 2, shade = TRUE)

AIC(m_0, m_1, m_2)

# Check the models
    gam.check(m_0)
    gam.check(m_1)
    
    #summary(mA)$s.table
    m_1$optimizer
    summary(m_1)$sp.criterion
    
    # ACF function:
    acf(resid(m_1), main="acf(resid(m_1))")

# 2. Effects
    #plot(m, pages = 1, scheme = 2, shade = TRUE, scale = 0)
    plot(m_1, pages = 1, scheme = 2, shade = TRUE)
    #plot(m_1, shade = TRUE)
    
    #ToD and MoY
    vis.gam(m_1, view=c("Year","MoY"), n.grid = 50, 
            theta = 35, phi = 32, zlab = "",
            ticktype = "detailed", color = "topo", main = "t2(D, W)")
    vis.gam(m_1, view=c("Year","MoY"), main = "t2(D, W)", plot.type = "contour",
            color = "terrain", contour.col = "black", lwd = 2)


# 3. Predictions 

# 3.2 Long-term trends in more detail ->  predict for particular stations and months 
# chose (midday, on month of year 1, 4, 7, and 10, at the selected station, and evaluating the long-term trend at 30 equally spaced values)

pdata <- with(WS.data,
              expand.grid(ToD = 12,
                          MoY = seq(1, 12, by = 1),
                          Year = seq(min(Year), max(Year), by = 1),
                          Zone = unique(Zone),
                          Sub_region  = "UK"))
fit <- data.frame(predict(m_1, newdata = pdata, se.fit = TRUE))
fit <- transform(fit, upper = fit + (2 * se.fit), lower = fit - (2 * se.fit))
pred <- cbind(pdata, fit)

ggplot(pred, aes(x = Year, y = fit, group = factor(MoY))) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = 'grey', alpha = 0.5) +
  geom_line() + facet_wrap(~ MoY) +
  labs(x = NULL, y = expression(Temperature ~ (degree * C)))

ggplot(pred, aes(x = MoY, y = fit, colour = factor(Year))) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = 'grey', alpha = 0.5) +
  geom_line() + facet_wrap(~Zone)+
  labs(x = NULL, y = expression(Temperature ~ (degree * C)))

```

## Omega

## pCO2

**RESULTS: **
* Seasonality 

## pH

**RESULTS: **
* ng years are bigger in summer-fall and smaller in winter-spring


## nDIC

**RESULTS: **
* ?????


## nTA

# Ofshore+Oceanic data 

## Temperature

## Omega

## pCO2

## pH

**RESULTS: **

## nDIC

**RESULTS: **
* ?????


## nTA

# Reef 

### Temperature

## Omega

## pCO2

## pH

**RESULTS: **

## nDIC

**RESULTS: **
* ?????

## nTA
